{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb4464c7-f2ae-401a-b691-2f00b0f83099",
   "metadata": {},
   "source": [
    "## Q1. What is an activation function in the context of artificial neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d8d0a-5b53-4719-9a95-a0b35e0bfc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Non-Linearity: Activation functions introduce non-linear transformations to the input data, allowing the network to model non-linear relationships between input and output variables. Without non-linear activation functions, neural networks would be limited to representing linear functions, severely limiting their expressive power.\n",
    "\n",
    "Normalization: Activation functions help to normalize the output of neurons, ensuring that the output remains within a certain range. This normalization helps stabilize the training process and prevents the output of neurons from becoming too large or too small, which can lead to issues such as vanishing or exploding gradients during backpropagation.\n",
    "\n",
    "Feature Learning: By introducing non-linearities, activation functions enable neural networks to learn complex features and patterns in the data. This allows neural networks to perform tasks such as image recognition, natural language processing, and speech recognition, where the underlying relationships are highly non-linear.\n",
    "\n",
    "Some common activation functions used in artificial neural networks include:\n",
    "\n",
    "Sigmoid: Sigmoid functions squash the output of neurons to a range between 0 and 1, making them suitable for binary classification tasks. However, they suffer from the vanishing gradient problem and are rarely used in deeper networks.\n",
    "\n",
    "Hyperbolic Tangent (tanh): Tanh functions squash the output of neurons to a range between -1 and 1, making them suitable for classification and regression tasks. Tanh functions are similar to sigmoid functions but have a larger output range, which can help speed up convergence during training.\n",
    "\n",
    "Rectified Linear Unit (ReLU): ReLU functions set negative inputs to zero and pass positive inputs unchanged, making them computationally efficient and easier to train compared to sigmoid and tanh functions. ReLU functions are widely used in deep learning architectures due to their simplicity and effectiveness.\n",
    "\n",
    "Leaky ReLU: Leaky ReLU functions allow a small, non-zero gradient for negative inputs, helping to mitigate the dying ReLU problem where neurons become inactive during training. Leaky ReLU functions can improve the stability and performance of deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3801d2-1545-4bf7-af47-a8f3fb6fe6b9",
   "metadata": {},
   "source": [
    "## Q2. What are some common types of activation functions used in neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a40e6-1d73-405c-a74c-fece4722b176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
